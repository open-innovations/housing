{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/lukestrange/Code/housing')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "ROOT = Path('../..')\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_geographies = pd.DataFrame()\n",
    "paths = [\n",
    "    \"metadata/lookups/Local_Authority_Districts_(April_2023)_Names_and_Codes_in_the_United_Kingdom.csv\", \n",
    "    \"metadata/lookups/Metropolitan_Counties_(December_2023)_Names_and_Codes_in_EN.csv\", \n",
    "    \"metadata/lookups/Regions_(December_2023)_Names_and_Codes_in_EN.csv\",\n",
    "    \"metadata/lookups/Combined_Authorities_(May_2024)_Names_and_Codes_in_England.csv\",\n",
    "    \"metadata/lookups/Counties_(April_2023)_Names_and_Codes_in_EN.csv\",\n",
    "    \"metadata/lookups/Countries_(December_2023)_Names_and_Codes_in_the_UK.csv\"\n",
    "    ]\n",
    "for path in paths:\n",
    "    data = pd.read_csv(ROOT / path)\n",
    "    code_name = data.columns[data.columns.str.endswith('CD')].values[0]\n",
    "    geo_name = data.columns[data.columns.str.endswith('NM')].values[0]\n",
    "    data.rename(columns={f'{code_name}': 'geography_code', f'{geo_name}': 'geography_name'}, inplace=True)\n",
    "    data = data[['geography_code', 'geography_name']]\n",
    "    active_geographies = pd.concat([active_geographies, data])\n",
    "\n",
    "active_geographies = active_geographies[~active_geographies['geography_code'].str.startswith(('W', 'S', 'N', 'K'))]\n",
    "active_geographies.reset_index(inplace=True, drop=True)\n",
    "active_geographies['active'] = True\n",
    "active_geographies.set_index('geography_code', inplace=True)\n",
    "active_geographies.to_json(ROOT / 'metadata/temp/active_geographies.json', orient='index', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [ROOT / 'data/vacant-homes/standard/AllCombined_Cleaned_2024.csv', ROOT / 'data/house-prices/standard/median_house_prices.csv', ROOT / 'data/affordable-homes/standard/by_tenure.csv']\n",
    "inactive_geographies = pd.DataFrame(columns=['geography_code', 'geography_name'])\n",
    "for file in files:\n",
    "    # Read the file\n",
    "    d = pd.read_csv(file)\n",
    "    \n",
    "    columns = d.columns.to_list()\n",
    "    assert 'geography_code' in columns, 'No column geography_code'\n",
    "    assert 'geography_name' in columns, 'No column geography_name'\n",
    "    \n",
    "    # # Group the names and codes to get unique combinations, drop the size column.\n",
    "    g = d.groupby(['geography_code', 'geography_name']).size().reset_index().drop(columns=0)\n",
    "    # g.reset_index(inplace=True)\n",
    "    # fix some known naming bugs.\n",
    "    g['geography_name'] = g['geography_name'].str.replace('&', 'and')\n",
    "    g['geography_name'] = g['geography_name'].str.replace('St Edmundsbury', 'St. Edmundsbury')\n",
    "\n",
    "    # For now, we only want place in England as this is the data we have.\n",
    "    g = g[g.geography_code.str.startswith('E')]\n",
    "    \n",
    "    # Ensure no duplicates remain\n",
    "    g.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Get lists of unique codes and names in the current dataset\n",
    "    unique_active_codes = active_geographies.index.unique()\n",
    "    unique_active_names = active_geographies['geography_name'].unique()\n",
    "    df_A = active_geographies.reset_index().drop(columns='active')\n",
    "    df_B = g\n",
    "    # Merge DataFrames with indicator to show the source of each row\n",
    "    merged_df = df_B.merge(df_A, how='left', indicator=True)\n",
    "\n",
    "    # Filter rows that are only in DataFrame B\n",
    "    unique_to_B = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "    # Drop the _merge column\n",
    "    unique_to_B = unique_to_B.drop(columns='_merge')\n",
    "    inactive_geographies = pd.concat([unique_to_B, inactive_geographies])\n",
    "\n",
    "# Set the active status remaining geographies to false\n",
    "inactive_geographies['active'] = False\n",
    "\n",
    "inactive_geographies.set_index('geography_code', inplace=True, drop=True)\n",
    "# Drop any duplicates that came from multiple files\n",
    "inactive_geographies.drop_duplicates(inplace=True)\n",
    "inactive_geographies.to_json(ROOT / 'metadata/temp/inactive_geographies.json', orient='index', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the frames and write to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains no duplicates...\n",
      " writing to JSON file.\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([active_geographies, inactive_geographies])\n",
    "dupes = combined[combined.index.duplicated()]\n",
    "if dupes.empty:\n",
    "    print('Contains no duplicates...\\n writing to JSON file.')\n",
    "    combined.to_json(ROOT / \"src/data/areas/place-page/_data/areas.json\", orient='index', indent=4)\n",
    "else: \n",
    "    print(dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the standard files. Fidn the first and last published dates. write it to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [ROOT / 'data/vacant-homes/standard/AllCombined_Cleaned_2024.csv', ROOT / 'data/house-prices/standard/median_house_prices.csv', ROOT / 'data/affordable-homes/standard/by_tenure.csv']\n",
    "# joined = pd.DataFrame()\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     d = pd.read_csv(file)\n",
    "#     assert 'Measure' in d.columns\n",
    "\n",
    "#     group = d.groupby(['geography_code', 'geography_name', 'Measure'])\n",
    "        \n",
    "#     min_dates = group['date'].min()\n",
    "#     max_dates = group['date'].max()\n",
    "#     g = group.size().reset_index().drop(columns=0)\n",
    "#     unique_min_dates = min_dates.unique()\n",
    "#     unique_max_dates = max_dates.unique()\n",
    "\n",
    "#     if len(unique_min_dates) == 1:\n",
    "#         min_date = unique_min_dates[0]\n",
    "#         max_date = unique_max_dates[0]\n",
    "#         g['fP'] = min_date\n",
    "#         g['lP'] = max_date\n",
    "#     else:\n",
    "#         g['lP'] = max_dates.reset_index()['date']\n",
    "#         g['fP'] = min_dates.reset_index()['date']\n",
    "\n",
    "#     g = g.pivot(index=['geography_code', 'geography_name'], columns='Measure', values=['fP', 'lP'])\n",
    "#     g.columns = g.columns.map('_'.join)\n",
    "#     g = g.reset_index().set_index('geography_code')\n",
    "#     # for i, row in g.iterrows():\n",
    "#     #     if i in joined.index:\n",
    "#     #         print(i)\n",
    "#     #         joined = pd.merge(joined, g, how='inner')\n",
    "#     #     else:\n",
    "#     #         joined = pd.concat([joined, g])\n",
    "#     g.to_json(f'blah{i}.json', orient='index', indent=4)\n",
    "#     i += 1\n",
    "# joined[joined.index == 'E06000001']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing-2Roxq_cV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
